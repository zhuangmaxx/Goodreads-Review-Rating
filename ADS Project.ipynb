{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33a2512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probably don't need this\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa99ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i think you'll have to move this file onto hpc first? \n",
    "import dask.dataframe as dd\n",
    "ddf = dd.read_json(\"goodreads_reviews_dedup.json\",lines=True,nrows=10000)\n",
    "df = ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3dcb73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>24375664</td>\n",
       "      <td>5cd416f3efc3f944fce4ce2db2290d5e</td>\n",
       "      <td>5</td>\n",
       "      <td>Mind blowingly cool. Best science fiction I've...</td>\n",
       "      <td>Fri Aug 25 13:55:02 -0700 2017</td>\n",
       "      <td>Mon Oct 09 08:55:59 -0700 2017</td>\n",
       "      <td>Sat Oct 07 00:00:00 -0700 2017</td>\n",
       "      <td>Sat Aug 26 00:00:00 -0700 2017</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>18245960</td>\n",
       "      <td>dfdbb7b0eb5a7e4c26d59a937e2e5feb</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>Sun Jul 30 07:44:10 -0700 2017</td>\n",
       "      <td>Wed Aug 30 00:00:26 -0700 2017</td>\n",
       "      <td>Sat Aug 26 12:05:52 -0700 2017</td>\n",
       "      <td>Tue Aug 15 13:23:18 -0700 2017</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>6392944</td>\n",
       "      <td>5e212a62bced17b4dbe41150e5bb9037</td>\n",
       "      <td>3</td>\n",
       "      <td>I haven't read a fun mystery book in a while a...</td>\n",
       "      <td>Mon Jul 24 02:48:17 -0700 2017</td>\n",
       "      <td>Sun Jul 30 09:28:03 -0700 2017</td>\n",
       "      <td>Tue Jul 25 00:00:00 -0700 2017</td>\n",
       "      <td>Mon Jul 24 00:00:00 -0700 2017</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>22078596</td>\n",
       "      <td>fdd13cad0695656be99828cd75d6eb73</td>\n",
       "      <td>4</td>\n",
       "      <td>Fun, fast paced, and disturbing tale of murder...</td>\n",
       "      <td>Mon Jul 24 02:33:09 -0700 2017</td>\n",
       "      <td>Sun Jul 30 10:23:54 -0700 2017</td>\n",
       "      <td>Sun Jul 30 15:42:05 -0700 2017</td>\n",
       "      <td>Tue Jul 25 00:00:00 -0700 2017</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>6644782</td>\n",
       "      <td>bd0df91c9d918c0e433b9ab3a9a5c451</td>\n",
       "      <td>4</td>\n",
       "      <td>A fun book that gives you a sense of living in...</td>\n",
       "      <td>Mon Jul 24 02:28:14 -0700 2017</td>\n",
       "      <td>Thu Aug 24 00:07:20 -0700 2017</td>\n",
       "      <td>Sat Aug 05 00:00:00 -0700 2017</td>\n",
       "      <td>Sun Jul 30 00:00:00 -0700 2017</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id   book_id  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d  24375664   \n",
       "1  8842281e1d1347389f2ab93d60773d4d  18245960   \n",
       "2  8842281e1d1347389f2ab93d60773d4d   6392944   \n",
       "3  8842281e1d1347389f2ab93d60773d4d  22078596   \n",
       "4  8842281e1d1347389f2ab93d60773d4d   6644782   \n",
       "\n",
       "                          review_id  rating  \\\n",
       "0  5cd416f3efc3f944fce4ce2db2290d5e       5   \n",
       "1  dfdbb7b0eb5a7e4c26d59a937e2e5feb       5   \n",
       "2  5e212a62bced17b4dbe41150e5bb9037       3   \n",
       "3  fdd13cad0695656be99828cd75d6eb73       4   \n",
       "4  bd0df91c9d918c0e433b9ab3a9a5c451       4   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  Mind blowingly cool. Best science fiction I've...   \n",
       "1  This is a special book. It started slow for ab...   \n",
       "2  I haven't read a fun mystery book in a while a...   \n",
       "3  Fun, fast paced, and disturbing tale of murder...   \n",
       "4  A fun book that gives you a sense of living in...   \n",
       "\n",
       "                       date_added                    date_updated  \\\n",
       "0  Fri Aug 25 13:55:02 -0700 2017  Mon Oct 09 08:55:59 -0700 2017   \n",
       "1  Sun Jul 30 07:44:10 -0700 2017  Wed Aug 30 00:00:26 -0700 2017   \n",
       "2  Mon Jul 24 02:48:17 -0700 2017  Sun Jul 30 09:28:03 -0700 2017   \n",
       "3  Mon Jul 24 02:33:09 -0700 2017  Sun Jul 30 10:23:54 -0700 2017   \n",
       "4  Mon Jul 24 02:28:14 -0700 2017  Thu Aug 24 00:07:20 -0700 2017   \n",
       "\n",
       "                          read_at                      started_at  n_votes  \\\n",
       "0  Sat Oct 07 00:00:00 -0700 2017  Sat Aug 26 00:00:00 -0700 2017       16   \n",
       "1  Sat Aug 26 12:05:52 -0700 2017  Tue Aug 15 13:23:18 -0700 2017       28   \n",
       "2  Tue Jul 25 00:00:00 -0700 2017  Mon Jul 24 00:00:00 -0700 2017        6   \n",
       "3  Sun Jul 30 15:42:05 -0700 2017  Tue Jul 25 00:00:00 -0700 2017       22   \n",
       "4  Sat Aug 05 00:00:00 -0700 2017  Sun Jul 30 00:00:00 -0700 2017        8   \n",
       "\n",
       "   n_comments  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           4  \n",
       "4           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #make sure it loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d27f6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all required imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "598445fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "df = df[df['rating'] != 0]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df[['review_text','rating']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3b028b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mind blowingly cool. Best science fiction I've...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I haven't read a fun mystery book in a while a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fun, fast paced, and disturbing tale of murder...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A fun book that gives you a sense of living in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  rating\n",
       "0  Mind blowingly cool. Best science fiction I've...       5\n",
       "1  This is a special book. It started slow for ab...       5\n",
       "2  I haven't read a fun mystery book in a while a...       3\n",
       "3  Fun, fast paced, and disturbing tale of murder...       4\n",
       "4  A fun book that gives you a sense of living in...       4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98043711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#rest of this came from the wine medium article\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Text cleaning\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320209b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_text_tokenized'] = df['review_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ad434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and convert to sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['review_text_tokenized'])\n",
    "sequences = tokenizer.texts_to_sequences(df['review_text_tokenized'])\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 100  # Maximum sequence length to pad\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['rating'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e7c22c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "241/241 [==============================] - 41s 144ms/step - loss: 1.9015 - val_loss: 0.9789\n",
      "Epoch 2/10\n",
      "241/241 [==============================] - 32s 135ms/step - loss: 0.7647 - val_loss: 0.8471\n",
      "Epoch 3/10\n",
      "241/241 [==============================] - 32s 132ms/step - loss: 0.4553 - val_loss: 0.8396\n",
      "Epoch 4/10\n",
      "241/241 [==============================] - 32s 132ms/step - loss: 0.3298 - val_loss: 0.8546\n",
      "Epoch 5/10\n",
      "241/241 [==============================] - 34s 139ms/step - loss: 0.2445 - val_loss: 0.8698\n",
      "Epoch 6/10\n",
      "241/241 [==============================] - 33s 137ms/step - loss: 0.1962 - val_loss: 0.9202\n",
      "Epoch 7/10\n",
      "241/241 [==============================] - 32s 134ms/step - loss: 0.1705 - val_loss: 0.9026\n",
      "Epoch 8/10\n",
      "241/241 [==============================] - 33s 135ms/step - loss: 0.1486 - val_loss: 0.8987\n",
      "Epoch 9/10\n",
      "241/241 [==============================] - 32s 135ms/step - loss: 0.1286 - val_loss: 0.9149\n",
      "Epoch 10/10\n",
      "241/241 [==============================] - 33s 136ms/step - loss: 0.1150 - val_loss: 0.9129\n",
      "61/61 [==============================] - 4s 33ms/step\n",
      "Mean Squared Error: 0.9129\n"
     ]
    }
   ],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_sequence_length))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=True)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac4e7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_model/trained_model_on_first_10k.keras') #i think this will save it to hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "432c12c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mind blowingly cool. Best science fiction I've read in some time. I just loved all the descriptions of the society of the future - how they lived in trees, the notion of owning property or even getting married was gone. How every surface was a screen. \n",
      " The undulations of how society responds to the Trisolaran threat seem surprising to me. Maybe its more the Chinese perspective, but I wouldn't have thought the ETO would exist in book 1, and I wouldn't have thought people would get so over-confident in our primitive fleet's chances given you have to think that with superior science they would have weapons - and defenses - that would just be as rifles to arrows once were. \n",
      " But the moment when Luo Ji won as a wallfacer was just too cool. I may have actually done a fist pump. Though by the way, if the Dark Forest theory is right - and I see no reason why it wouldn't be - we as a society should probably stop broadcasting so much signal out into the universe.\n"
     ]
    }
   ],
   "source": [
    "#dont need this\n",
    "vals = df['review_text'].tolist()\n",
    "print(vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1f30691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mind blowingly cool. Best science fiction I've...</td>\n",
       "      <td>5</td>\n",
       "      <td>[mind, blowingly, cool, best, science, fiction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>5</td>\n",
       "      <td>[special, book, started, slow, first, third, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  rating  \\\n",
       "0  Mind blowingly cool. Best science fiction I've...       5   \n",
       "1  This is a special book. It started slow for ab...       5   \n",
       "\n",
       "                               review_text_tokenized  \n",
       "0  [mind, blowingly, cool, best, science, fiction...  \n",
       "1  [special, book, started, slow, first, third, m...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2) #ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cfc68be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted Score: 4.35\n"
     ]
    }
   ],
   "source": [
    "#this is to just make sure it worked \n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = load_model('trained_model/trained_model_on_first_10k.keras')\n",
    "\n",
    "def predict_review_score(review_text: str) -> float:\n",
    "    # Preprocess the review text\n",
    "    sequence = tokenizer.texts_to_sequences([review_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
    "\n",
    "    # Make the prediction using the loaded model\n",
    "    predicted_score = loaded_model.predict(padded_sequence)[0][0]\n",
    "\n",
    "    return predicted_score\n",
    "\n",
    "# Sample review to predict the score\n",
    "sample_review = \"Mind blowingly cool. Best science fiction I've read in some time. I just loved all the descriptions of the society of the future - how they lived in trees, the notion of owning property or even getting married was gone. How every surface was a screen. The undulations of how society responds to the Trisolaran threat seem surprising to me. Maybe its more the Chinese perspective, but I wouldn't have thought the ETO would exist in book 1, and I wouldn't have thought people would get so over-confident in our primitive fleet's chances given you have to think that with superior science they would have weapons - and defenses - that would just be as rifles to arrows once were. But the moment when Luo Ji won as a wallfacer was just too cool. I may have actually done a fist pump. Though by the way, if the Dark Forest theory is right - and I see no reason why it wouldn't be - we as a society should probably stop broadcasting so much signal out into the universe.\"\n",
    "# Call the function to predict the score\n",
    "predicted_score = predict_review_score(sample_review)\n",
    "print(f\"Predicted Score: {predicted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358259c",
   "metadata": {},
   "source": [
    "### Ignore everything below this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e8a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting allennlp==0.4.2\n",
      "  Obtaining dependency information for allennlp==0.4.2 from https://files.pythonhosted.org/packages/c5/60/fa613bdea022bd6c26176f5786efcc0b5a6d8acf97131e324179a99fcbaf/allennlp-0.4.2-py3-none-any.whl.metadata\n",
      "  Using cached allennlp-0.4.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 0.2.0 Requires-Python ==3.6\n",
      "ERROR: Could not find a version that satisfies the requirement torch==0.3.1 (from allennlp) (from versions: 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1)\n",
      "ERROR: No matching distribution found for torch==0.3.1\n"
     ]
    }
   ],
   "source": [
    "pip install allennlp==0.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e7db62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==0.3.1 (from versions: 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1)\n",
      "ERROR: No matching distribution found for torch==0.3.1\n"
     ]
    }
   ],
   "source": [
    "pip install torch==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4e3bd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hubNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for tensorflow-hub from https://files.pythonhosted.org/packages/e5/50/00dba77925bf2a0a1e45d7bcf8a69a1d2534fb4bb277d9010bd148d2235e/tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-hub) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-hub) (4.25.5)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow-hub)\n",
      "  Obtaining dependency information for tf-keras>=2.14.1 from https://files.pythonhosted.org/packages/8a/ed/e08afca471299b04a34cd548e64e89d0153eda0e6cf9b715356777e24774/tf_keras-2.18.0-py3-none-any.whl.metadata\n",
      "  Using cached tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.19,>=2.18 (from tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Obtaining dependency information for tensorflow<2.19,>=2.18 from https://files.pythonhosted.org/packages/cf/24/271e77c22724f370c24c705f394b8035b4d27e4c2c6339f3f45ab9b8258e/tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Obtaining dependency information for tensorflow-intel==2.18.0 from https://files.pythonhosted.org/packages/76/ad/fa6c508a15ff79cb5409294c293388e0999b7d480f84b65e4287277434fe/tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (23.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.67.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Obtaining dependency information for tensorboard<2.19,>=2.18 from https://files.pythonhosted.org/packages/b1/de/021c1d407befb505791764ad2cbd56ceaaa53a746baed01d2e2143f05f18/tensorboard-2.18.0-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Obtaining dependency information for keras>=3.5.0 from https://files.pythonhosted.org/packages/c2/88/eef50051a772dcb4433d1f3e4c1d6576ba450fe83e89d028d7e8b85a2122/keras-3.6.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\harin\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (13.9.3)\n",
      "Requirement already satisfied: namex in c:\\users\\harin\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\harin\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harin\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harin\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harin\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harin\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\harin\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\harin\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\harin\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\harin\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.1.0)\n",
      "Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Using cached tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl (390.2 MB)\n",
      "Using cached keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: tensorboard, keras, tensorflow-intel, tensorflow, tf-keras, tensorflow-hub\n",
      "Successfully installed keras-3.6.0 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-hub-0.16.1 tensorflow-intel-2.18.0 tf-keras-2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\harin\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\harin\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-hub --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8091ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pretrained ELMo model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pretrained ELMo model\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_embed_sentence(sentence):\n",
    "    # Convert sentence to a TensorFlow tensor\n",
    "    embeddings = elmo.signatures['default'](tf.constant([sentence]))\n",
    "    # Get the ELMo embedding (shape: [1, sequence_length, 1024])\n",
    "    elmo_embeddings = embeddings['elmo']\n",
    "    return elmo_embeddings[0].numpy()  # Convert to a NumPy array for easier handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['elmo_embeddings'] = df['text_column'].apply(elmo_embed_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
