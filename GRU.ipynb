{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will preprocess 100k instances of the data and then train the GRU model for 10 epochs. This requires the dataset google drive"
      ],
      "metadata": {
        "id": "2CmHJk-1XwEW"
      },
      "id": "2CmHJk-1XwEW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a0a16b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cfef71e-22a2-4c4b-b379-5d58785d451d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import dask.dataframe as dd\n",
        "ddf = dd.read_json(\"/content/drive/MyDrive/goodreads_reviews_dedup.json\",lines=True,nrows=100000)\n",
        "df = ddf.compute()"
      ],
      "id": "6a0a16b2"
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary packages\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "a5lVIR08cHek"
      },
      "id": "a5lVIR08cHek",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping erroenous values\n",
        "df = df[df['rating'] != 0]\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df = df[['review_text','rating']]"
      ],
      "metadata": {
        "id": "ItTWdtCOcKiH"
      },
      "id": "ItTWdtCOcKiH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adapted from this source: https://medium.com/@jozsef.dudas/predicting-wine-review-scores-from-text-using-lstm-bcfdbf7b4c6\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text: str) -> str:\n",
        "    # Text cleaning\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stop word removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "UX9SDJTHcUHc"
      },
      "id": "UX9SDJTHcUHc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review_text_tokenized'] = df['review_text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "2bwDQgCYcdEU"
      },
      "id": "2bwDQgCYcdEU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adapted from this source: https://medium.com/@jozsef.dudas/predicting-wine-review-scores-from-text-using-lstm-bcfdbf7b4c6\n",
        "\n",
        "# Tokenize the text and convert to sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['review_text_tokenized'])\n",
        "sequences = tokenizer.texts_to_sequences(df['review_text_tokenized'])\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = 100  # Maximum sequence length to pad\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['rating'], test_size=0.2)"
      ],
      "metadata": {
        "id": "Go38DET3cd6a"
      },
      "id": "Go38DET3cd6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can train the GRU!"
      ],
      "metadata": {
        "id": "_dnvhwFIchm9"
      },
      "id": "_dnvhwFIchm9"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=64))\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(1, activation='linear'))"
      ],
      "metadata": {
        "id": "9cb-Y59tSZkU"
      },
      "id": "9cb-Y59tSZkU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_squared_error'])"
      ],
      "metadata": {
        "id": "-W4l7DeqSiku"
      },
      "id": "-W4l7DeqSiku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    batch_size=64,\n",
        "                    epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cjynu8cStRn",
        "outputId": "903cecb3-885f-4252-8780-d475cf64f59a"
      },
      "id": "8cjynu8cStRn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 1.9433 - mean_squared_error: 1.9433 - val_loss: 0.6992 - val_mean_squared_error: 0.6992\n",
            "Epoch 2/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.6333 - mean_squared_error: 0.6333 - val_loss: 0.6692 - val_mean_squared_error: 0.6692\n",
            "Epoch 3/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.5582 - mean_squared_error: 0.5582 - val_loss: 0.6533 - val_mean_squared_error: 0.6533\n",
            "Epoch 4/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.5131 - mean_squared_error: 0.5131 - val_loss: 0.6622 - val_mean_squared_error: 0.6622\n",
            "Epoch 5/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.4656 - mean_squared_error: 0.4656 - val_loss: 0.7377 - val_mean_squared_error: 0.7377\n",
            "Epoch 6/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.4115 - mean_squared_error: 0.4115 - val_loss: 0.7005 - val_mean_squared_error: 0.7005\n",
            "Epoch 7/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.3517 - mean_squared_error: 0.3517 - val_loss: 0.6958 - val_mean_squared_error: 0.6958\n",
            "Epoch 8/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.2968 - mean_squared_error: 0.2968 - val_loss: 0.7629 - val_mean_squared_error: 0.7629\n",
            "Epoch 9/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.2582 - mean_squared_error: 0.2582 - val_loss: 0.7791 - val_mean_squared_error: 0.7791\n",
            "Epoch 10/10\n",
            "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.2222 - mean_squared_error: 0.2222 - val_loss: 0.7770 - val_mean_squared_error: 0.7770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse = model.evaluate(X_test, y_test, verbose=0)[1]  # Get MSE\n",
        "print(f'Test Mean Squared Error: {mse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEaEhNgkS0HZ",
        "outputId": "7eebbefe-a550-43e9-ae35-75c21027bc44"
      },
      "id": "oEaEhNgkS0HZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Mean Squared Error: 0.7722707986831665\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}